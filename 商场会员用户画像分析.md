# 商场会员用户画像分析

该报告着重对数据建模后分析，对数据预处理这块只放了一部分的截图和代码

## 1.数据获取

数据源两个：会员信息表，销售流水表

![image-20250813000140388](C:\Users\Johnny\AppData\Roaming\Typora\typora-user-images\image-20250813000140388.png)

![image-20250813000144197](C:\Users\Johnny\AppData\Roaming\Typora\typora-user-images\image-20250813000144197.png)

## 2.数据预处理

```python
# 会员信息表去重
df_cum.drop_duplicates(subset = '会员卡号', inplace = True)
print('会员卡号（去重）有{}条记录'.format(len(df_cum['会员卡号'].unique())))

# 去除登记时间的缺失值
df_cum.dropna(subset = ['登记时间'], inplace = True)
print('df_cum（去重和去缺失）有{}条记录'.format(df_cum.shape[0]))
```



![image-20250813000237252](C:\Users\Johnny\AppData\Roaming\Typora\typora-user-images\image-20250813000237252.png)



```python
# 由于出生日期这一列的缺失值过多，且存在较多的异常值，不能贸然删除
# 故下面另建一个数据集L来保存“出生日期”和“性别”信息，方便下面对会员的性别和年龄信息进行统计
L = pd.DataFrame(df_cum.loc[df_cum['出生日期'].notnull(), ['出生日期', '性别']])
L['年龄'] = L['出生日期'].astype(str).apply(lambda x: x[:3] + '0')
L.drop('出生日期', axis = 1, inplace = True)
L['年龄'].value_counts()
# 用于与销售流水表进行合并的数据只取['会员卡号', '性别', '登记时间']这三列，将出生日期这列意义不大的进行删除（这列信息最有可能出错），并重置索引
df_cum.drop('出生日期', axis = 1, inplace = True)
df_cum.index = range(df_cum.shape[0])
print('数据清洗之后共有{}行记录，{}列字段，字段分别为{}'.format(df_cum.shape[0], df_cum.shape[1], df_cum.columns.tolist()))
```

![image-20250813000406330](C:\Users\Johnny\AppData\Roaming\Typora\typora-user-images\image-20250813000406330.png)

## 3.数据分析及可视化

### 3.1 会员的出生年代和男女比例分析

从下图可以看出出生年代集中于1960-1990年，女会员比例81%比男会员19%比例大

![image-20250813000714278](C:\Users\Johnny\AppData\Roaming\Typora\typora-user-images\image-20250813000714278.png)

### 3.2 会员年龄分布情况（中年、青年、老年）

\1. 可以看出商场会员的年龄主要是中年为主（1960-1990）

![image-20250813000735013](C:\Users\Johnny\AppData\Roaming\Typora\typora-user-images\image-20250813000735013.png)

### 3.3 会员与非会员的总订单和总消费金额占比

由下图可以看出总订单占比非会员占比多，但是总消费金额会员多

![image-20250813000753118](C:\Users\Johnny\AppData\Roaming\Typora\typora-user-images\image-20250813000753118.png)

### 3.4 分析不同时间段会员的消费时间偏好（用户消费频次（订单数）越多）

由下图可以分析出第二季度会有更多人来消费，月末消费订单数最多

![image-20250813000818565](C:\Users\Johnny\AppData\Roaming\Typora\typora-user-images\image-20250813000818565.png)

### 3.5 分析2015-2018年会员的季度消费订单和月内消费订单

还是可以得到月末消费订单多，2015年之后，随着季度增加，消费订单变多

![image-20250813000844549](C:\Users\Johnny\AppData\Roaming\Typora\typora-user-images\image-20250813000844549.png)

### 3.6 分析各小时段的销售订单数

可以分析出傍晚订单数是最多的

![image-20250813000902387](C:\Users\Johnny\AppData\Roaming\Typora\typora-user-images\image-20250813000902387.png)

![image-20250813001016878](C:\Users\Johnny\AppData\Roaming\Typora\typora-user-images\image-20250813001016878.png)

## 4.特征工程

会员用户画像模型构建LRFMP

L: 入会程度（新用户、中等用户、老用户）

R: 最近购买的时间（月）

F: 消费频数（低频、中频、高频）

M: 消费总金额（高消费、中消费、低消费）

P: 积分（高、中、低）

S: 消费时间偏好（凌晨、上午、中午、下午、晚上）

X：性别

![image-20250813000950502](C:\Users\Johnny\AppData\Roaming\Typora\typora-user-images\image-20250813000950502.png)

![image-20250813000954430](C:\Users\Johnny\AppData\Roaming\Typora\typora-user-images\image-20250813000954430.png)

## 5. 数据建模及评估

### 5.1 K-Means算法

#### 轮廓系数

```python
# 对数据进行聚类
n_clusters = range(2, 7)
scores = []
for i in range(len(n_clusters)):
    clf = KMeans(n_clusters = n_clusters[i], random_state = 20).fit(res_std)
    scores.append(silhouette_score(res_std, clf.labels_))
maxindex = scores.index(max(scores))
plt.figure(figsize = (8, 6), dpi = 100) 
plt.plot(n_clusters, scores, linestyle = '-.', c = 'b', alpha = 0.6, marker = 'o')
plt.axvline(x = n_clusters[maxindex], linestyle = '--', c = 'r', alpha = 0.5)
plt.title('LRFMP的聚类轮廓系数图')
plt.ylabel('silhouette_score')
plt.xlabel('n_clusters')
plt.savefig('./LRFMP聚类轮廓系数图.png')
```

![image-20250813001229031](C:\Users\Johnny\AppData\Roaming\Typora\typora-user-images\image-20250813001229031.png)

图中可以看出当K=5时，轮廓系数为45%，聚类效果最好

#### 训练模型

```python
# 以聚类数为5贴上对应的标签
clf = KMeans(n_clusters = 5, random_state = 20).fit(res_std)
df0['labels'] = clf.labels_
df0
```

![image-20250813001328610](C:\Users\Johnny\AppData\Roaming\Typora\typora-user-images\image-20250813001328610.png)

#### 图像分析

![image-20250813001645913](C:\Users\Johnny\AppData\Roaming\Typora\typora-user-images\image-20250813001645913.png)

从上图看出

类别 0：数量 22041，占比 51.80%，为主要用户群体，各项指标处于中等水平

类别 1：数量 8482，占比 19.94%，老用户占比最多，可通过发放优惠券刺激消费

类别 2：数量 231，占比 0.54%，消费频率、总金额及积分远高于其他类别，定义为重要保持用户

类别 3：数量 1984，占比 4.66%，消费频率和金额处于中等，为一般发展用户

类别 4：数量 9810，占比 23.06%，具有一定规模，需进一步挖掘其消费潜力

### 5.2 PCA高特征降维

通过 PCA 降维 + KMeans（n_clusters=2）将用户分为 2 类（标签 0 和 1 ），并对比了两类在 L、R、F、M、P 五个维度的均值差异

```python
res_std1 = StandardScaler().fit_transform(df0)
pca = PCA(n_components=0.95, random_state=20)  # 保留95%方差，减少噪声
```

![image-20250813001745281](C:\Users\Johnny\AppData\Roaming\Typora\typora-user-images\image-20250813001745281.png)

![image-20250813001835406](C:\Users\Johnny\AppData\Roaming\Typora\typora-user-images\image-20250813001835406.png)

（1）L（入会时长 / 会员年限）

 “会员年限” 不是区分价值的核心因素。可能商场用户整体入会时间相近，或新老用户的消费习惯差异不大。

（2）R（最近消费间隔）

标签 1 的 R 值 远小于 标签 0 → 说明 标签 1 客户的 “最近消费时间” 更接近当前时间

（3）F（消费频率）

解标签 1 的 F 值 远高于 标签 0 → 说明 标签 1 客户的消费频率显著更高

标签 1 客户 “消费间隔短、频率高”，更可能是 高价值 / 活跃用户

（4）M（消费总金额）

标签 1 的 M 值 远高于 标签 0 → 说明 标签 1 客户的累计消费金额高

高频 + 高消费 → 标签 1 更可能是 高价值用户

（5）P（积分）

标签 1 的 P 值 远高于 标签 0 → 积分通常与消费金额正相关

标签 1 是 高价值用户（高消费 → 高积分 ）。



### 5.3 使用DBSCAN算法

```python
dbscan = DBSCAN(eps=0.8, min_samples=10, n_jobs=-1)  # n_jobs=-1使用所有CPU
df0['labels_dbscan'] = dbscan.fit_predict(res_std1)

# 查看聚类结果（-1表示噪声点）
print("DBSCAN聚类标签分布：")
print(df0['labels_dbscan'].value_counts())
```

![image-20250813002003587](C:\Users\Johnny\AppData\Roaming\Typora\typora-user-images\image-20250813002003587.png)



分析与结论

| ***\*指标\****              | ***\*异常客户均值\**** | ***\*正常客户均值\**** | ***\*业务\****                                         |
| --------------------------- | ---------------------- | ---------------------- | ------------------------------------------------------ |
| ***\*L（入会时长）\****     | 52.7                   | 43                     | 异常客户是 “更老的用户”，忠诚度基础更好                |
| ***\*R（最近消费间隔）\**** | 3.98                   | 13                     | 异常客户 “3 天前刚消费”，比正常客户（13 天前）活跃得多 |
| ***\*F（消费频率）\****     | 63.01                  | 4                      | 异常客户 “消费频率是正常用户的 15 倍”，高频复购        |
| ***\*M（消费总金额）\****   | 252849.74              | 10719                  | 异常客户 “消费金额是正常用户的 23 倍”，高客单价        |
| ***\*P（积分）\****         | 223900.67              | 9516                   | 积分和消费金额强相关，进一步验证高消费                 |

由上图可以看出DBSCAN 把这些用户标记为 “噪声点”，但实际是最有价值的核心用户
DBSCAN 认为这些用户的特征 “太特殊、密度太低”（比如消费金额远高于其他人，被算法当成 “离群点”），但从业务看，这正是需要重点运营的优质客户。

或许还是用回KMeans ,不会把高价值用户当噪声，适合这种场景



## 6.总结

当用PCA降维后当K值为2时轮廓系数最接近于1，但是一开始我们用了K=5，K=5的话就把客户分的更细，K=2就快速聚焦两类用户，因此具体还需要看业务要“精细分层”还是“快速聚焦”如果 K=5 时，某一类用户复购率特别高，且在 LRFMP 特征上有独特表现，能对应到业务里 “高频复购用户” 群体，就说明聚类有价值 

使用DBSCAN算法，该算法将一些特征当作为异常点，所以可能该场景不适合用DBSCAN算法